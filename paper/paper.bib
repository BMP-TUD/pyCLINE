@article{Brunton:2016,
    author = {Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
    doi = {10.1073/pnas.1517384113},
    issn = {0027-8424},
    journal = {Proceedings of the National Academy of Sciences},
    language = {eng},
    month = {apr},
    number = {15},
    pages = {3932--3937},
    title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
    volume = {113},
    year = {2016}
}

@article{Cranmer:2019,
    archivePrefix = {arXiv},
    eprint = {1909.05862},
    author = {Cranmer, Miles D. and Xu, Rui and Battaglia, Peter and Ho, Shirley},
    journal = {ArXiv},
    month = {sep},
    title = {Learning Symbolic Physics with Graph Networks},
    year = {2019}
}

@article{Karniadakis:2021,
    author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
    doi = {10.1038/s42254-021-00314-5},
    issn = {2522-5820},
    journal = {Nature Reviews Physics},
    month = {may},
    number = {6},
    pages = {422--440},
    publisher = {Springer Nature},
    title = {Physics-informed machine learning},
    volume = {3},
    year = {2021}
}

@article{Lagergren:2020,
    author = {Lagergren, John H. and Nardini, John T. and Baker, Ruth E. and Simpson, Matthew J. and Flores, Kevin B.},
    doi = {10.1371/journal.pcbi.1008462},
    editor = {Lavrik, Inna},
    issn = {1553-7358},
    journal = {PLOS Computational Biology},
    month = {dec},
    number = {12},
    pages = {e1008462},
    publisher = {Public Library of Science},
    title = {Biologically-informed neural networks guide mechanistic modeling from sparse experimental data},
    volume = {16},
    year = {2020}
}

@article{Murari:2020,
    author = {Murari, A. and Peluso, E. and Lungaroni, M. and Gaudio, P. and Vega, J. and Gelfusa, M.},
    doi = {10.1038/s41598-020-76826-4},
    issn = {2045-2322},
    journal = {Scientific Reports},
    month = {nov},
    number = {1},
    pages = {1--10},
    pmid = {33199734},
    publisher = {Nature Publishing Group},
    title = {Data driven theory for knowledge discovery in the exact sciences with applications to thermonuclear fusion},
    volume = {10},
    year = {2020}
}

@article{Prokop:2024,
    author = {Prokop, Bartosz and Gelens, Lendert},
    doi = {10.1016/j.isci.2024.109316},
    issn = {2589-0042},
    journal = {iScience},
    keywords = {Bioinformatics, Machine learning},
    month = {apr},
    number = {4},
    pages = {109316},
    publisher = {Elsevier},
    title = {From biological data to oscillator models using SINDy},
    volume = {27},
    year = {2024}
}

@article{ProkopB:2024,
    author = {Prokop, Bartosz and Frolov, Nikita and Gelens, Lendert},
    doi = {10.1063/5.0199311},
    issn = {1054-1500},
    journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
    month = {jun},
    number = {6},
    pages = {063135},
    publisher = {AIP Publishing},
    title = {Enhancing model identification with SINDy via nullcline reconstruction},
    volume = {34},
    year = {2024}
}

@article{Prokop:2025,
    author = {Prokop, Bartosz and Billen, Jimmy and Frolov, Nikita and Gelens, Lendert},
    title = {Uncovering hidden nullcline structures behind time series data using neural networks},
    archivePrefix = {arXiv},
    eprint = {XXXX.XXXXX},
    year = {2025}
}

@article{Haluszczynski:2019,
    archivePrefix = {arXiv},
    eprint = {1907.05639},
    author = {Haluszczynski, Alexander and R{\"a}th, Christoph},
    doi = {10.1063/1.5118725},
    issn = {1089-7682},
    journal = {Chaos},
    month = {oct},
    number = {10},
    pages = {103143},
    pmid = {31675800},
    publisher = {American Institute of Physics Inc.},
    title = {Good and bad predictions: Assessing and improving the replication of chaotic attractors by means of reservoir computing},
    volume = {29},
    year = {2019}
}

@article{Rackauckas2020,
    abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." In this manuscript we introduce the SciML software ecosystem as a tool for mixing the information of physical laws and scientific models with data-driven machine learning approaches. We describe a mathematical object, which we denote universal differential equations (UDEs), as the unifying framework connecting the ecosystem. We show how a wide variety of applications, from automatically discovering biological mechanisms to solving high-dimensional Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled through the UDE formalism and its tooling. We demonstrate the generality of the software tooling to handle stochasticity, delays, and implicit constraints. This funnels the wide variety of SciML applications into a core set of training mechanisms which are highly optimized, stabilized for stiff equations, and compatible with distributed parallelism and GPU accelerators.},
    archivePrefix = {arXiv},
    arxivId = {2001.04385},
    author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
    eprint = {2001.04385},
    month = {jan},
    title = {{Universal Differential Equations for Scientific Machine Learning}},
    url = {http://arxiv.org/abs/2001.04385},
    year = {2020}
}


@book{billings2013nonlinear,
    address = {Chichester, UK},
    author = {Billings, Stephen A},
    doi = {10.1002/9781118535561},
    isbn = {9781118535561},
    mendeley-groups = {Paper Limit Cycle},
    month = {jul},
    publisher = {John Wiley & Sons, Ltd},
    title = {{Nonlinear System Identification}},
    year = {2013}
}

@article{Paszke2019,
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
eprint = {1912.01703},
journal = {NeurIPS},
month = {dec},
number = {NeurIPS},
pages = {8026--8037},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://arxiv.org/abs/1912.01703},
year = {2019}
}
